{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84e53e6-a805-4437-b690-abf822e31fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # or 'roberta-base' if you want native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aba63fa-8e0b-4830-ab4c-3db25c6581e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>is_self</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>selftext</th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>poll_data</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>title</th>\n",
       "      <th>over_18</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>locked</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>I'm writing a bunch of articles on the topic o...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>negative</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>[-0.030923799, -0.055583965, 0.009136942, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "      <td>We created **Leval-S**, a new way to measure *...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1kqzha5</td>\n",
       "      <td>LatterEquivalent8478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>We benchmarked gender bias across top LLMs (GP...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>We benchmarked gender bias across top LLMs (GP...</td>\n",
       "      <td>[0.0052795396, -0.08025029, -0.021995582, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>So here’s the deal: I needed a 3D icon ASAP. N...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1jdko1z</td>\n",
       "      <td>Creepy_Effective_598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>Almost lost it over a 3D icon, but AI saved th...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>166</td>\n",
       "      <td>negative</td>\n",
       "      <td>Almost lost it over a 3D icon, but AI saved th...</td>\n",
       "      <td>[-0.10449125, -0.0026527045, -0.010666771, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "      <td>Honestly, using a Mac with Apple Silicon sucks...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>14iid79</td>\n",
       "      <td>luxuryBubbleGum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>RANT: I hate Apple Silicon</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>RANT: I hate Apple Silicon Honestly, using a M...</td>\n",
       "      <td>[-0.033601113, -0.07439321, 0.037087034, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>45</td>\n",
       "      <td># How come LLM responds in constant time even ...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1evuwr6</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>If you think LLMs can reason and plan, please ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>negative</td>\n",
       "      <td>If you think LLMs can reason and plan, please ...</td>\n",
       "      <td>[-0.0140480595, 0.0054782196, -0.0011468532, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  is_self  num_comments  \\\n",
       "0           0     True            86   \n",
       "1           1     True            19   \n",
       "2           7     True             7   \n",
       "3           9     True            29   \n",
       "4          10     True            45   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I'm writing a bunch of articles on the topic o...   \n",
       "1  We created **Leval-S**, a new way to measure *...   \n",
       "2  So here’s the deal: I needed a 3D icon ASAP. N...   \n",
       "3  Honestly, using a Mac with Apple Silicon sucks...   \n",
       "4  # How come LLM responds in constant time even ...   \n",
       "\n",
       "                                                 url       id  \\\n",
       "0  https://www.reddit.com/r/deeplearning/comments...  1e3qyxd   \n",
       "1  https://www.reddit.com/r/deeplearning/comments...  1kqzha5   \n",
       "2  https://www.reddit.com/r/deeplearning/comments...  1jdko1z   \n",
       "3  https://www.reddit.com/r/deeplearning/comments...  14iid79   \n",
       "4  https://www.reddit.com/r/deeplearning/comments...  1evuwr6   \n",
       "\n",
       "                 author  link_flair_text  poll_data  upvote_ratio  \\\n",
       "0   Difficult-Race-1188              NaN        NaN          0.53   \n",
       "1  LatterEquivalent8478              NaN        NaN          0.50   \n",
       "2  Creepy_Effective_598              NaN        NaN          0.65   \n",
       "3       luxuryBubbleGum              NaN        NaN          0.54   \n",
       "4   Difficult-Race-1188              NaN        NaN          0.57   \n",
       "\n",
       "                                               title  over_18  \\\n",
       "0  Scale Won’t Turn LLMs Into AGI or Superintelli...    False   \n",
       "1  We benchmarked gender bias across top LLMs (GP...    False   \n",
       "2  Almost lost it over a 3D icon, but AI saved th...    False   \n",
       "3                         RANT: I hate Apple Silicon    False   \n",
       "4  If you think LLMs can reason and plan, please ...    False   \n",
       "\n",
       "   author_flair_text  locked  score     label  \\\n",
       "0                NaN   False     13  negative   \n",
       "1                NaN   False      0  negative   \n",
       "2                NaN   False    166  negative   \n",
       "3                NaN   False      4  negative   \n",
       "4                NaN   False     10  negative   \n",
       "\n",
       "                                      title_selftext  \\\n",
       "0  Scale Won’t Turn LLMs Into AGI or Superintelli...   \n",
       "1  We benchmarked gender bias across top LLMs (GP...   \n",
       "2  Almost lost it over a 3D icon, but AI saved th...   \n",
       "3  RANT: I hate Apple Silicon Honestly, using a M...   \n",
       "4  If you think LLMs can reason and plan, please ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.030923799, -0.055583965, 0.009136942, 0.03...  \n",
       "1  [0.0052795396, -0.08025029, -0.021995582, 0.04...  \n",
       "2  [-0.10449125, -0.0026527045, -0.010666771, -0....  \n",
       "3  [-0.033601113, -0.07439321, 0.037087034, -0.05...  \n",
       "4  [-0.0140480595, 0.0054782196, -0.0011468532, -...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = pd.read_csv(\"deeplearning_submissions_controversial_labeled.csv\")\n",
    "posts = posts[~posts['selftext'].isna()]\n",
    "posts = posts[posts['label'].notna()]\n",
    "posts['title_selftext'] = posts.apply(\n",
    "    lambda p: (p['title'] if pd.notnull(p['title']) else '') + ' ' + (p['selftext'] if pd.notnull(p['selftext']) else ''),\n",
    "    axis=1\n",
    ")\n",
    "posts['embeddings'] = posts['title_selftext'].map(lambda p: encoder.encode(p))\n",
    "posts['label'] = posts['label'].str.lower()\n",
    "print(posts.shape)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b688e1-bde1-4d7c-8336-7f1eec120ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id_y</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score_y</th>\n",
       "      <th>body</th>\n",
       "      <th>depth</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ldaipgn</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>69</td>\n",
       "      <td>There's no actual factual content here, just h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.07071394, -0.030216975, 0.021154435, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ld9ybjt</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>41</td>\n",
       "      <td>None of this features any actual scientific cl...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0051783687, -0.0032895522, 0.018412238, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ldaocb4</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>10</td>\n",
       "      <td>How is it possible for this garbage to have 30...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.060968414, -0.070379905, -0.025280824, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ldam55b</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>9</td>\n",
       "      <td>Your ideas are immature and incomplete with to...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.02631399, 0.05668127, 0.0043701353, -0.0719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lda9xol</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>6</td>\n",
       "      <td>That paper reads like an opinion piece rather ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[-0.15103108, 0.0047795195, -0.013959803, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id_y parent_id  score_y  \\\n",
       "0           0  ldaipgn   1e3qyxd       69   \n",
       "1           1  ld9ybjt   1e3qyxd       41   \n",
       "2           2  ldaocb4   1e3qyxd       10   \n",
       "3           3  ldam55b   1e3qyxd        9   \n",
       "4           4  lda9xol   1e3qyxd        6   \n",
       "\n",
       "                                                body  depth  \\\n",
       "0  There's no actual factual content here, just h...      1   \n",
       "1  None of this features any actual scientific cl...      2   \n",
       "2  How is it possible for this garbage to have 30...      3   \n",
       "3  Your ideas are immature and incomplete with to...      4   \n",
       "4  That paper reads like an opinion piece rather ...      5   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.07071394, -0.030216975, 0.021154435, 0.002...  \n",
       "1  [-0.0051783687, -0.0032895522, 0.018412238, -0...  \n",
       "2  [-0.060968414, -0.070379905, -0.025280824, 0.0...  \n",
       "3  [0.02631399, 0.05668127, 0.0043701353, -0.0719...  \n",
       "4  [-0.15103108, 0.0047795195, -0.013959803, -0.0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv(\"deeplearning_submissions_comments_controversial.csv\")\n",
    "comments = comments[comments['parent_id'].isin(posts['id'])]\n",
    "# Derive column depth\n",
    "comments['depth'] = comments.groupby('parent_id').cumcount() + 1\n",
    "comments['embeddings'] = comments['body'].map(lambda c: encoder.encode(c))\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7caa94-97bd-4b20-ac22-a24d057afa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>is_self</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>selftext</th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>poll_data</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>embeddings_x</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>id_y</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score_y</th>\n",
       "      <th>body</th>\n",
       "      <th>depth</th>\n",
       "      <th>embeddings_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>I'm writing a bunch of articles on the topic o...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>[-0.030923799, -0.055583965, 0.009136942, 0.03...</td>\n",
       "      <td>0</td>\n",
       "      <td>ldaipgn</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>69</td>\n",
       "      <td>There's no actual factual content here, just h...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.07071394, -0.030216975, 0.021154435, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>I'm writing a bunch of articles on the topic o...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>[-0.030923799, -0.055583965, 0.009136942, 0.03...</td>\n",
       "      <td>1</td>\n",
       "      <td>ld9ybjt</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>41</td>\n",
       "      <td>None of this features any actual scientific cl...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0051783687, -0.0032895522, 0.018412238, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>I'm writing a bunch of articles on the topic o...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>[-0.030923799, -0.055583965, 0.009136942, 0.03...</td>\n",
       "      <td>2</td>\n",
       "      <td>ldaocb4</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>10</td>\n",
       "      <td>How is it possible for this garbage to have 30...</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.060968414, -0.070379905, -0.025280824, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>I'm writing a bunch of articles on the topic o...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>[-0.030923799, -0.055583965, 0.009136942, 0.03...</td>\n",
       "      <td>3</td>\n",
       "      <td>ldam55b</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>9</td>\n",
       "      <td>Your ideas are immature and incomplete with to...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.02631399, 0.05668127, 0.0043701353, -0.0719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>I'm writing a bunch of articles on the topic o...</td>\n",
       "      <td>https://www.reddit.com/r/deeplearning/comments...</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>Difficult-Race-1188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Scale Won’t Turn LLMs Into AGI or Superintelli...</td>\n",
       "      <td>[-0.030923799, -0.055583965, 0.009136942, 0.03...</td>\n",
       "      <td>4</td>\n",
       "      <td>lda9xol</td>\n",
       "      <td>1e3qyxd</td>\n",
       "      <td>6</td>\n",
       "      <td>That paper reads like an opinion piece rather ...</td>\n",
       "      <td>5</td>\n",
       "      <td>[-0.15103108, 0.0047795195, -0.013959803, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0_x  is_self  num_comments  \\\n",
       "0             0     True            86   \n",
       "1             0     True            86   \n",
       "2             0     True            86   \n",
       "3             0     True            86   \n",
       "4             0     True            86   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I'm writing a bunch of articles on the topic o...   \n",
       "1  I'm writing a bunch of articles on the topic o...   \n",
       "2  I'm writing a bunch of articles on the topic o...   \n",
       "3  I'm writing a bunch of articles on the topic o...   \n",
       "4  I'm writing a bunch of articles on the topic o...   \n",
       "\n",
       "                                                 url       id  \\\n",
       "0  https://www.reddit.com/r/deeplearning/comments...  1e3qyxd   \n",
       "1  https://www.reddit.com/r/deeplearning/comments...  1e3qyxd   \n",
       "2  https://www.reddit.com/r/deeplearning/comments...  1e3qyxd   \n",
       "3  https://www.reddit.com/r/deeplearning/comments...  1e3qyxd   \n",
       "4  https://www.reddit.com/r/deeplearning/comments...  1e3qyxd   \n",
       "\n",
       "                author  link_flair_text  poll_data  upvote_ratio  ...  \\\n",
       "0  Difficult-Race-1188              NaN        NaN          0.53  ...   \n",
       "1  Difficult-Race-1188              NaN        NaN          0.53  ...   \n",
       "2  Difficult-Race-1188              NaN        NaN          0.53  ...   \n",
       "3  Difficult-Race-1188              NaN        NaN          0.53  ...   \n",
       "4  Difficult-Race-1188              NaN        NaN          0.53  ...   \n",
       "\n",
       "      label                                     title_selftext  \\\n",
       "0  negative  Scale Won’t Turn LLMs Into AGI or Superintelli...   \n",
       "1  negative  Scale Won’t Turn LLMs Into AGI or Superintelli...   \n",
       "2  negative  Scale Won’t Turn LLMs Into AGI or Superintelli...   \n",
       "3  negative  Scale Won’t Turn LLMs Into AGI or Superintelli...   \n",
       "4  negative  Scale Won’t Turn LLMs Into AGI or Superintelli...   \n",
       "\n",
       "                                        embeddings_x  Unnamed: 0_y     id_y  \\\n",
       "0  [-0.030923799, -0.055583965, 0.009136942, 0.03...             0  ldaipgn   \n",
       "1  [-0.030923799, -0.055583965, 0.009136942, 0.03...             1  ld9ybjt   \n",
       "2  [-0.030923799, -0.055583965, 0.009136942, 0.03...             2  ldaocb4   \n",
       "3  [-0.030923799, -0.055583965, 0.009136942, 0.03...             3  ldam55b   \n",
       "4  [-0.030923799, -0.055583965, 0.009136942, 0.03...             4  lda9xol   \n",
       "\n",
       "  parent_id score_y                                               body  depth  \\\n",
       "0   1e3qyxd      69  There's no actual factual content here, just h...      1   \n",
       "1   1e3qyxd      41  None of this features any actual scientific cl...      2   \n",
       "2   1e3qyxd      10  How is it possible for this garbage to have 30...      3   \n",
       "3   1e3qyxd       9  Your ideas are immature and incomplete with to...      4   \n",
       "4   1e3qyxd       6  That paper reads like an opinion piece rather ...      5   \n",
       "\n",
       "                                        embeddings_y  \n",
       "0  [-0.07071394, -0.030216975, 0.021154435, 0.002...  \n",
       "1  [-0.0051783687, -0.0032895522, 0.018412238, -0...  \n",
       "2  [-0.060968414, -0.070379905, -0.025280824, 0.0...  \n",
       "3  [0.02631399, 0.05668127, 0.0043701353, -0.0719...  \n",
       "4  [-0.15103108, 0.0047795195, -0.013959803, -0.0...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_thread = pd.merge(posts, comments, left_on=\"id\", right_on=\"parent_id\")\n",
    "print(df_thread.shape)\n",
    "df_thread.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4edc4-351c-48c0-8308-fcc49fcd2617",
   "metadata": {},
   "source": [
    "# First baseline: Weighted average of VADER scores across comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728e7775-4d47-4fd2-a2aa-74ee92f0f345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.1091\n",
      "1    0.8703\n",
      "2    0.3612\n",
      "3   -0.4209\n",
      "4    0.3612\n",
      "Name: polarity_score_compound, dtype: float64\n",
      "count    600.000000\n",
      "mean       0.251334\n",
      "std        0.447315\n",
      "min       -0.940300\n",
      "25%        0.000000\n",
      "50%        0.221700\n",
      "75%        0.624900\n",
      "max        0.995400\n",
      "Name: polarity_score_compound, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_thread['polarity_score_compound'] = df_thread['body'].map(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "print(df_thread['polarity_score_compound'].head())\n",
    "print(df_thread['polarity_score_compound'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a88e68-0bf1-47f8-b41c-9e10fba6be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts['vader_weighted_mean'] = 0.0\n",
    "# for i in range(posts.shape[0]):\n",
    "#     post_id = posts.iloc[i].id\n",
    "#     post_comments = comments[comments['parent_id'] == post_id]\n",
    "#      # Handle edge case to avoid division by zero\n",
    "#     if post_comments['score_y'].sum() == 0 or post_comments.empty:\n",
    "#         weighted_mean = 0.0\n",
    "#     else:\n",
    "#         weighted_mean = (post_comments['polarity_score_compound'] * post_comments['score_y']).sum() / post_comments['score_y'].sum()\n",
    "#     posts.at[i, 'vader_weighted_mean'] = weighted_mean\n",
    "        \n",
    "# posts['vader_weighted_mean'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89dca756-373f-491d-9e07-6304b9ca66af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted_avg_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.202354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.351492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.052547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.270056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.496862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.883200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       weighted_avg_polarity\n",
       "count              90.000000\n",
       "mean                0.202354\n",
       "std                 0.351492\n",
       "min                -0.922700\n",
       "25%                -0.052547\n",
       "50%                 0.270056\n",
       "75%                 0.496862\n",
       "max                 0.883200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def safe_weighted_avg(x):\n",
    "    total_weight = x['score_y'].sum()\n",
    "    if total_weight == 0:\n",
    "        return 0.0  # or np.nan, depending on what makes more sense\n",
    "    return (x['polarity_score_compound'] * x['score_y']).sum() / total_weight\n",
    "\n",
    "grouped_polarity_weighted = df_thread.groupby('parent_id').apply(safe_weighted_avg, include_groups=False).reset_index(name='weighted_avg_polarity')\n",
    "\n",
    "grouped_polarity_weighted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c324c3-c2dc-4ab8-8a75-f1bc1f7198d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weighted_avg_polarity column\n",
    "posts = posts.merge(grouped_polarity_weighted, left_on='id', right_on='parent_id', how='left')\n",
    "posts = posts.drop(columns=['parent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d125490-c931-4915-980b-b94407b51cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count           90\n",
       "unique           3\n",
       "top       positive\n",
       "freq            57\n",
       "Name: sentiment_label, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heuristic to convert continuous values into labels\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "posts['sentiment_label'] = posts['weighted_avg_polarity'].apply(classify_sentiment)\n",
    "posts['sentiment_label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6904a57-0163-4ae1-8c15-45e46cc957e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35555555555555557"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vader Accuracy\n",
    "sum(posts['label'].str.lower() == posts['sentiment_label'].str.lower())/posts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77efb0c-8fb8-4705-90f8-4b3bc25ebe69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Second baseline: Flat RoBERTa + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "365c9fa5-7d66-471a-ba69-b93e3b673067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all comments\n",
    "comments_grouped = comments.groupby('parent_id')['body'].apply(lambda texts: ' '.join(texts)).reset_index(name='all_comments')\n",
    "posts = posts.merge(comments_grouped, left_on='id', right_on='parent_id', how='left')\n",
    "posts = posts.drop(columns=['parent_id'])\n",
    "# TODO: Add tags f\"[POST] {row['title']} {row['selftext']} [COMMENTS] {row['all_comments']}\"\n",
    "posts['full_text'] = posts['title'].fillna('') + ' ' + posts['selftext'].fillna('') + ' ' + posts['all_comments'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae78a2d6-2355-48e2-a705-c2131f86ce4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df565f3cede14894aa452e918493378f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get embeddings of full text\n",
    "embeddings = encoder.encode(posts['all_comments'].fillna(''), show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344ebafe-82d6-4bb9-888d-5c9dce2f54c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.80      0.67         5\n",
      "     neutral       0.60      0.67      0.63         9\n",
      "    positive       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.56        18\n",
      "   macro avg       0.39      0.49      0.43        18\n",
      "weighted avg       0.46      0.56      0.50        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, posts['label'], test_size=0.2, stratify=posts['label'], random_state=42)\n",
    "\n",
    "clf = SVC(kernel='linear')  # or 'rbf'\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1943e6a-edf3-4ddc-a3fc-9341a7b3d40c",
   "metadata": {},
   "source": [
    "# Proposed model\n",
    "# HAN + RoBERTa + vote/depth-aware attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5529988-fd41-4c01-ab3d-1406dee1ae5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'is_self', 'num_comments', 'selftext', 'url', 'id',\n",
       "       'author', 'link_flair_text', 'poll_data', 'upvote_ratio', 'title',\n",
       "       'over_18', 'author_flair_text', 'locked', 'score', 'label',\n",
       "       'title_selftext', 'embeddings', 'weighted_avg_polarity',\n",
       "       'sentiment_label', 'all_comments', 'full_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14bdf77c-6729-47b2-b716-55b203092f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Suppose `posts` is your dataframe and `labels` is a NumPy array or a column\n",
    "# train_df, val_df = train_test_split(posts, test_size=0.2, stratify=posts['label'], random_state=42)\n",
    "\n",
    "# # Create datasets and loaders\n",
    "# train_dataset = YourCustomDataset(train_df)\n",
    "# val_dataset = YourCustomDataset(val_df)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=..., shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=..., shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7316dd31-d514-4ef4-ba6d-7b89f8f92b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'is_self', 'num_comments', 'selftext', 'url', 'id',\n",
       "       'author', 'link_flair_text', 'poll_data', 'upvote_ratio', 'title',\n",
       "       'over_18', 'author_flair_text', 'locked', 'score', 'label',\n",
       "       'title_selftext', 'embeddings', 'weighted_avg_polarity',\n",
       "       'sentiment_label', 'all_comments', 'full_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ef9ee0-3580-4ea1-a7b2-45862ccae40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_length(matrix_list, max_len, embed_dim, pad_value=0.0):\n",
    "    padded = np.full((len(matrix_list), max_len, embed_dim), pad_value, dtype=np.float32)\n",
    "    for i, mat in enumerate(matrix_list):\n",
    "        length = min(len(mat), max_len)\n",
    "        padded[i, :length] = mat[:length]\n",
    "    return padded\n",
    "\n",
    "def pad_metadata(meta_list, max_len, pad_value=0.0):\n",
    "    padded = np.full((len(meta_list), max_len, 1), pad_value, dtype=np.float32)\n",
    "    for i, m in enumerate(meta_list):\n",
    "        length = min(len(m), max_len)\n",
    "        padded[i, :length, 0] = m[:length]\n",
    "    return padded\n",
    "\n",
    "posts['comment_embedding_matrix'] = posts['id'].\\\n",
    "    map(lambda p_id: \n",
    "            np.array(\n",
    "                [c['embeddings'] for _, c in comments[comments['parent_id'] == p_id].iterrows()]\n",
    "            )\n",
    "       )\n",
    "\n",
    "\n",
    "# 1. Extract raw comment embeddings into list of 2D arrays (one per post)\n",
    "comments_embeddings_list = [\n",
    "    np.array([c['embeddings'] for _, c in comments[comments['parent_id'] == post_id].iterrows()])\n",
    "    for post_id in posts['id']\n",
    "]\n",
    "\n",
    "# 2. Do the same for scores and depths\n",
    "comments_scores_list = [\n",
    "    np.array([c['score_y'] for _, c in comments[comments['parent_id'] == post_id].iterrows()])\n",
    "    for post_id in posts['id']\n",
    "]\n",
    "\n",
    "posts['comment_scores'] = posts['id'].\\\n",
    "    map(lambda p_id: \n",
    "            np.array(\n",
    "                [c['score_y'] for _, c in comments[comments['parent_id'] == p_id].iterrows()]\n",
    "            )\n",
    "       )\n",
    "\n",
    "comments_depths_list = [\n",
    "    np.array([c['depth'] for _, c in comments[comments['parent_id'] == post_id].iterrows()])\n",
    "    for post_id in posts['id']\n",
    "]\n",
    "\n",
    "posts['comment_depths'] = posts['id'].\\\n",
    "    map(lambda p_id: \n",
    "            np.array(\n",
    "                [c['depth'] for _, c in comments[comments['parent_id'] == p_id].iterrows()]\n",
    "            )\n",
    "       )\n",
    "\n",
    "# Max number of comments per post\n",
    "max_len = max(len(x) for x in posts['comment_embedding_matrix'])\n",
    "embed_dim = 384  # sentence encoder embedding size\n",
    "\n",
    "# Padding\n",
    "comments_embeddings_padded = pad_to_max_length(comments_embeddings_list, max_len, embed_dim)\n",
    "scores_padded = pad_metadata(comments_scores_list, max_len)\n",
    "depths_padded = pad_metadata(comments_depths_list, max_len)\n",
    "\n",
    "posts['comment_embedding_matrix_padded'] = pad_to_max_length(posts['comment_embedding_matrix'].values, max_len, embed_dim)\n",
    "posts['comments_scores_padded'] = pad_metadata(posts['comment_scores'], max_len)\n",
    "posts['comments_depths_padded'] = pad_metadata(posts['comment_depths'], max_len)\n",
    "posts['embeddings'] = np.stack(posts['embeddings'].values)  # assuming this column exists\n",
    "\n",
    "# Labels\n",
    "label_to_id = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2\n",
    "}\n",
    "\n",
    "posts['label_id'] = posts['label'].map(label_to_id)\n",
    "# Post embeddings as array\n",
    "post_embeddings = posts['embeddings']  # assuming this column exists\n",
    "\n",
    "labels = posts['label_id'].values  # or 'manual_label' or whatever column you use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8237de68-1623-4564-9a49-3bdf5486a8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 23, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04ffeab6-5369-4e66-9257-5aa13e2f4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CommentAttentionWithMetadata(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attn_weights = nn.Linear(embed_dim + 2, 1)  # +2 for score and depth\n",
    "\n",
    "    def forward(self, embeddings, scores, depths):\n",
    "        # embeddings: (batch_size, seq_len, embed_dim)\n",
    "        # scores, depths: (batch_size, seq_len, 1)\n",
    "        x = torch.cat([embeddings, scores, depths], dim=-1)  # shape: (batch, seq, embed+2)\n",
    "        attn_logits = self.attn_weights(x).squeeze(-1)  # shape: (batch, seq)\n",
    "        attn_weights = torch.softmax(attn_logits, dim=1)\n",
    "        context = torch.sum(embeddings * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return context, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "659d9f85-8f4b-40b4-8973-ce435e97b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANWithCommentMetadata(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.attn = CommentAttentionWithMetadata(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim + embed_dim, num_classes)  # post + comment vector\n",
    "\n",
    "    def forward(self, post_embed, comment_embeds, scores, depths):\n",
    "        # All tensors should be batched (batch_size, ...)\n",
    "        comment_context, _ = self.attn(comment_embeds, scores, depths)\n",
    "        combined = torch.cat([post_embed, comment_context], dim=-1)\n",
    "        output = self.classifier(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e12fa26-cf1e-495e-85df-6e9a4da5787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RedditHANData(Dataset):\n",
    "    def __init__(self, post_embeddings, comment_embeddings, scores, depths, labels):\n",
    "        self.post_embeddings = torch.tensor(post_embeddings, dtype=torch.float32)\n",
    "        self.comment_embeddings = torch.tensor(comment_embeddings, dtype=torch.float32)\n",
    "        self.scores = torch.tensor(scores, dtype=torch.float32)\n",
    "        self.depths = torch.tensor(depths, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.post_embeddings[idx],\n",
    "                self.comment_embeddings[idx],\n",
    "                self.scores[idx],\n",
    "                self.depths[idx],\n",
    "                self.labels[idx])\n",
    "\n",
    "# Create DataLoaders\n",
    "dataset = RedditHANData(post_embeddings, comments_embeddings_padded, \n",
    "                        scores_padded, depths_padded, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6b1909b-92a4-4376-bc3a-66c407c68db1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = HANWithCommentMetadata(embed_dim=\u001b[43mpost_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m, num_classes=\u001b[32m3\u001b[39m)\n\u001b[32m      5\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m      6\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = HANWithCommentMetadata(embed_dim=post_embeddings.shape[1], num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf4c75-520d-449c-98e1-d7a5602ba162",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for post, comments, scores, depths, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(post, comments, scores, depths)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190676b-0c0b-4708-a56b-1de1a59b9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for post, comments, scores, depths, label in train_loader:  # use test_loader if split\n",
    "        outputs = model(post, comments, scores, depths)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d5aa2-3ca4-41dd-bf21-0e2e8e824106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
