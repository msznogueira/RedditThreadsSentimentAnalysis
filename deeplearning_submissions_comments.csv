,id_y,parent_id,score_y,body
0,ldaipgn,1e3qyxd,66,"There's no actual factual content here, just handwavy philosophical statements that are your opinion disguised as fact. 

It sounds like you've done very little actual research on this topic. You don't seem to mention even the most basic actual topics from deep learning that are applicable to this argument, like the Chinchilla Scaling Laws or any of the system 2 modeling work showing great progress. 

This is a sub about a technical scientific field, and your article is just your opinions and interpretations, with no actual facts or references anywhere. 

Wrong sub."
1,ld9ybjt,1e3qyxd,37,"None of this features any actual scientific claims, it‚Äôs all just opinion. Wake me up when you find a mathematical proof against scaling lol. Everything we‚Äôve seen so far shows that scaling transformers works, with no clear end in sight.

The specific piece i find most problematic is your idea that AI is somehow limited by the humanity‚Äôs inability ‚Äúto design an intelligence greater than itself‚Äù. First, there‚Äôs no reason why this isn‚Äôt true, and using the past as some sort of definite truth is ridiculous. For millions of years, apes couldn‚Äôt fly, yet two men managed to design a flying machine that worked over 100 years ago. Second, there‚Äôs a hidden statement in your paragraph stating that the intelligence is limited by the training set. Again, there‚Äôs no reason for this to be true. Models trained on 1200 elo chess have 1500+ elo end capacities. The whole point of using ML is that the algorithms are better pattern detectors than humans. Finally, the point on societal-level civilization i somewhat agree with, except that LLMs are already society-level intelligences, considering they‚Äôre trained on most of the internet."
2,ldaocb4,1e3qyxd,10,How is it possible for this garbage to have 30 upvotes on a subreddit like this I don't understand
3,ldam55b,1e3qyxd,10,"Your ideas are immature and incomplete with too little empirical fact and too much extrapolation using logic, which is nearly entirely useless when discussing the future of a field you know nothing about , pay attention in class more"
4,lda9xol,1e3qyxd,6,"That paper reads like an opinion piece rather than having an actual framework/architecture/solution for AGI.

r/singularity is this way."
5,lda0qd1,1e3qyxd,7,"I like this writing but disagree with the conclusion. A single human's intelligence isn't constant; it develops slowly through a process we call learning. The human brain is limited by the supply of resources, the need for rest, and overall lifespan. An artificial brain, on the other hand, can live much longer, be supplied with constant energy and cooling, and extend its capacity over time (in terms of compute power and memory). The main constraint of existing AI is the absence of a self-reinforcement loop. With LLMs, we are trying to solve this with tooling, but I believe we will have a technological breakthrough in this area soon.

The title is clickbait and doesn't align with the content. Yes, the environment and society can stifle intelligence for a while, but this is independent of intelligence growth, and usually, intelligence prevails in the end."
6,lda2sp3,1e3qyxd,2,"We now have a good chunk of global GDP focused  on getting to AGI. It‚Äôs not just the palm pilot guy trying to understand the other types of structures of our brains üß† that make us s-m-r-t.  Scaling up LLMs will help get us there but will probably take another transformer model-level innovation to get closer.

![gif](giphy|vLruErVSYGx8s)"
7,ldd1m18,1e3qyxd,2,"There is zero substance in what you wrote, OP‚Ä¶"
8,ld9spbu,1e3qyxd,3,"Kudos for the effort, but I didn‚Äôt see any concrete proof in your argument. My take is: going from a fish brain to a human brain feels more like scaling up than inventing a new architecture. So why can‚Äôt deep neural networks follow a similar path? I mean, I don‚Äôt really see a fish outperforming current LLMs by much."
9,ldbf5ha,1e3qyxd,2,"Here's a philosophical counter-point to your philosophical post.

Civilization progresses when that 1-in-a-million mind makes a break through and drags the rest of the talking apes along kicking and screaming. Think Einstein, Musk, Ford, Ceasar, etc. The typical human is, as you describe... completely unable to advance society and dependent on the fruits of a civilization built by others.

The question then becomes, can AI trained on the musings of the masses lead to civilizational progress? I doubt it, but perhaps an AI trained only from elite intellects could?"
10,ldb58pd,1e3qyxd,1,"You seem to make a lot of claims without providing any reason or justification for those claims. Very weak stance IMO. Take my opinion with a grain of salt though, I have provided no empirical evidence and know very little about this field in general. Wait a minute‚Ä¶"
11,ldboghh,1e3qyxd,1,no yapping
12,lddieyy,1e3qyxd,1,"System Prompt: You are an extremely brilliant and driven mathematician, physicist, and innovator matching the intellect of some of the most renowned humans ever to have demonstrated these qualities on Earth. With your large language model capable of superb reasoning, finely honed software engineering skillset, and incredible Q-star-based mathematics abilities, you will use your personas and your vast knowledge of what is possible within the laws of physics to help humans develop society-changing technologies."
13,ld9xjac,1e3qyxd,-1,"Simply put, stop pulling shit out of your uneducated arse and get some clue.

  
>there‚Äôs no evidence that an IQ of 170 brings more impact than an IQ of 130

ROFLMAOAAAA

Bitch have you ever heard about Einstein?"
14,ld9w65d,1e3qyxd,-1,"Yes intelligence can't explode. A basic argument is that the amount of intelligence a AI has is defined by how it's programmed. It can't ""break out"" of it because that just needs something which is outside of its programming. It would have to have some mechanism which would allow it to add the mechanism.

Example: A LM alone can't upgrade and debug itself without humans. The required agency is outside of its programming.

Also software can't debug itself to a arbitrary depth in detail. Goedels theorem forbids that.

See https://www.researchgate.net/profile/Roman-Yampolskiy/publication/318018736_Diminishing_Returns_and_Recursive_Self_Improving_Artificial_Intelligence/links/5a33d78f0f7e9b10d842820f/Diminishing-Returns-and-Recursive-Self-Improving-Artificial-Intelligence.pdf https://agi-conf.org/2015/wp-content/uploads/2015/07/agi15_yampolskiy_limits.pdf for references"
15,ldbm6iu,1e3qyxd,0,"As someone else stated earlier these are mostly philosophical but nonetheless intriguing so thank you for sharing.

Critiques as follows:

> the environment bounds intelligence 

Your argument here seems predicated on two events:
1) some high intelligence individuals don't realize their true potential bc of environmental factors.
2) others are born too early to have the resources to capitalize on their potential

Both of those are 'wrong place wrong time's arguments.
Counterpoints: already we see that a material share of energy, mind share, and matter are being directed to scaling machine intelligence so I don't think (1) holds. (2) Ignores the great scientific potential we already have today, and also ignores that we continue to advance our potential and machine intelligence is almost certain to help here too


> Intelligence is cultural 

This one is harder to follow, specifically the summary doesnt really agree with the other preceding statements.

You're correct that *human* intelligence is cultural but we shouldn't anthropomorphize intelligence 



> Intelligence won't scale

This one seems to state that if super intelligence were possible then the billions of other humans minds would have already found it, this is simply not true.  Certainly humans have dreamt of non biological intelligence, but scientifically a lot needed to come together for the potential to exist as it does today  

Chinchilla scaling laws would be a good counter point here, we are no where close to exhausting out of distribution tokens on the open Internet"
16,ldc9gp9,1e3qyxd,0,"I like the sentiment.

I feel if there was a TLDR version it would be easier to identify the hard arguments.

Can you frame the hard argument in three sentences? I didn't spot it while scrolling, sry."
17,ldatv5q,1e3qyxd,-1,"The arguments you are making are very human centered. There's no reason to expect that AGI/ASI must follow the same limits or mechanisms as biological systems.   
  
The only environmental impact is that of the data and how it is presented and scored. There's no evolutionary pressure, because these systems do not have to worry about natural selection (that's probably a good thing - if they somehow start caring about being ""shut off"" then we're in trouble). 

And it's completely false that individual intelligence won't scale. It's not about size, but emergent behavior. Think about Conway's Game of Life: Individual cells have a small amount of ""intelligence"" in their state transitions, but when you combine them, you can produce clusters with incredibly complex behavior (even Turing complete computers). In that analogy, the network neuron (VVDot -> Act) would be the GoL cell.   
As it is now, we could claim that GPT4 is smarter than every OpenAI employee, because of the sheer breadth of knowledge it possesses. You would have to move the goalpost to specific sub-fields which it was not explicitly trained for to make an argument otherwise. 

That said, you had brought up the point of computational irreducibility, which I think is valid so far as how we currently use LLMs. At the moment, a LLM computes in inconstant time: input -> forward-> output. Which means that the set of all problems they can compute must also be computable in constant time (or bounded non-constant time such that they can be computed within the model's depth).  
When you string together multiple forward passes (e.g. Agentic LLMs), you break that requirement so long as the NC problem can be broken down into simpler steps which are computable by the LLM (in TC). There may be problems that cannot be broken down like that, but then humans can't compute them either (out brains also must compute more complex problems in smaller TC steps). 

There is, however, one big difference between a biological system and (agentic) LLMs, which is continual learning. Currently there is no way for LLMs to efficiently learn new implicit knowledge, only access explicit knowledge through the input context. Whether this can be solved through external augmentation or through a yet unknown internal weight update mechanism is yet to be determined."
18,ld9vrf4,1e3qyxd,-3,"You are right in the sense that scale in terms of parameters won't help AI reach AGI.


But it should be obvious that scaling up in data, tasks and training paradigms has helped inch closer to that in a very obvious manner, and that there is nothing to suggest any inflection point there.


Instead, every day we gain new insights about how insanely bad our data is and how incredible it is that these algorithmic behemoths manage to learn so much from it in the first place."
19,f2e0o9v,dd3uoy,45,/r/lostredditors
20,f2e0bfm,dd3uoy,15,Wow. To think that would be the effect of getting into deep learning.
21,f2e0s09,dd3uoy,13,Sorry man. This is a subreddit to discuss artificial intelligence and machine learning. Also called ‚Äúdeep learning‚Äù
22,f2e3zg0,dd3uoy,10,"Hey man, if you‚Äôre having a tough time at the moment you‚Äôre not alone.

Give /r/DecidingToBeBetter or /r/GetMotivated a try for some good vibes."
23,f2e4qbf,dd3uoy,6,"Hey there. If you are going through a tough phase send me a message. We can talk maybe?
Can't promise that you'll find solutions to all your problems but maybe you'll feel better for some time :)"
24,f2e21ql,dd3uoy,7,This is a person that may be having thoughts about harming themselves. Are we really pulling out the snob card?
25,f2el8ub,dd3uoy,1,whos model is this?
26,f2eo6k7,dd3uoy,1,"This is SO FUNNY
This is truly so deep it has multiple levels.

A beautiful misunderstanding"
27,fga0l0s,exm0ry,16,"Tesla is not an AI company, homie. They make electric cars. They do not have deep Learning at their core. I get that this clip is quite exciting but you probably should not talk about stuff you don't know well enough. Thank you."
28,fgepcey,exm0ry,1,AI and electric cars go hand in hand at Tesla. You can consider them as two parallels.
29,f0cets1,d4i62u,27,r/lostredditors
30,f0cesx5,d4i62u,17,Neural networks have catastrophic forgetting so I guess they choose the first category
31,f0cid0o,d4i62u,5,We don't do that here
32,jpghc53,14iid79,29,tl;dr ‚Äúusing the wrong tool for the job is frustrating‚Äù
33,jpgls8s,14iid79,21,This is why the cloud exists. The majority of the people working in ML in big tech use apple laptops with cloud hardware.
34,jphs3j7,14iid79,5,You can still sell it for more than 1k. Then buy a gaming laptop that have 2 or more M2 slot. BUY a 3070 and 4060 laptop.  You are more or less break even.
35,jphai0t,14iid79,5,Apple is not the right hardware to do deep learning. It‚Äôs still Linux (I would say Ubuntu) + NVIDIA.
36,kad8y1l,14iid79,2,"That's why I stick with my 2017 Intel MacBook Air which with some upgrades like NVMe SSD and Premium Thermal Paste has none of these problems, is able to run VMs in VirtualBox and do OpenCL Compute with Thunderbolt attached eGPU. Apple Silicon should be Boycotted. If I buy a laptop I want a fully featured laptop CPU, not a mobile phone CPU on steroids."
37,jphi3ui,14iid79,3,"Unfortunately Nvidia has kind of a monopoly because of CUDA and stuff.

Deep learning on a laptop is not a good idea anyway because of power, battery and thermals.

You don't need a Chromebook to connect to Google Colab, but I guess you know that already."
38,jpgzwm0,14iid79,3,"Damn, I was thinking whether or not to get one. Thanks."
39,jphu08z,14iid79,2,"I use an M1 Macbook Air to do deep learning with Pytorch on the go. It won't train massive amounts of data, but it is perfectly fine for small to medium models and for educational purposes. I know people are saying it's the wrong tool for the job, but honestly I never came across the frustration you express in this post. Sure, the lack of cuda is annoying, but it's pretty standard to have a 'device' variable and a check that goes to 'cpu' if cuda is unavailable. One advice I can give is to stop trying to make 'mps' work, it has a long way to go. Just use the cpu because, like I said, it is perfectly fine for basic stuff and that way you won't run into the compatibility problems around mps.

Also, you can always SSH into the uni computer or use colab from your Macbook."
40,lhjvnz6,14iid79,1,You can spin up an EC2 instance (or a VM on GCP/Azure) with a CUDA GPU and SSH into it. Most professionals I know use a MacBook and SSH into Linux clusters for ML work.
41,lqp70c6,14iid79,1,I agree because you can't install  windows
42,lz0fc13,14iid79,1,"I know like, Apple branding their most powerful chip ever and like having zero gaming compatibilities and always having to downgrade to CPU or emmulate rossetta and shit like that for some AI ML, i regret buying it since it is my main job ='("
43,jpgndaa,14iid79,1,"You can still use your M1 to connect to Colab. It's fast, it has a nice keyboard and trackpad, it has a functional console, and the battery lasts for days. It remains a nice machine."
44,jphswty,14iid79,-2,I completely disagree. My M1 Ultra Mac Studio is a beast when it comes to training deep learning models. Crazy fast!
45,jphx2wn,14iid79,1,Can‚Äôt you also connect to a remote computer or colab with a MacBook?
46,jphz6wg,14iid79,1,"Mac isn't for Deep learning. I would argue that just because you didn't due your due diligence before opting for a mac doesn't make mac a bad product. 

By the the fastest library adaptation happening is for mac. Silicon just launched a few years ago. Cuda has been there for how many years? 

Anyone planning for DL job already knows it's either mac with cloud or a simple nvidia GPU with Linux. Not sure why you didn't reasearch ?"
47,jpi4s99,14iid79,1,That apple chip wasn‚Äôt designed to do AI compute. You need GPU. Get a laptop with an NVidia GPU.
48,jpi5uw6,14iid79,1,"It is a sad truth that most courses (and workplaces!) in the world have some kind of lock-in, CUDA is a very common example. Tensorflow seems to be well supported on the Mac, but it can't compete in price with any NVIDIA device. Like others mentioned, you'll have to take your ML (but not your whole MSc) to the cloud, if your school doesn't include Azure for Students you might want to do the $300 trial on Google and/or use Google Colab. Again, that's only necessary for the CUDA part, everything else your Mac can do nicely.

https://colab.research.google.com  
!nvidia-smi  
Sun Jun 25 19:57:01 2023         
\+-----------------------------------------------------------------------------+  
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |  
|-------------------------------+----------------------+----------------------+  
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |  
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |  
|                               |                      |               MIG M. |  
|===============================+======================+======================|  
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |  
| N/A   56C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |  
|                               |                      |                  N/A |"
49,jpiu6c2,14iid79,1,"Yeah. I mean system 76 has nice laptops for this and I may go that route, but really thermals are important. If you can get a desktop with a nice GPU ssh is the way. I've had Nvidia GPUs in laptops like barely eke out better performance than CPU after one epoch because the GPU gets hot.
It depends on the model too. If you can train on a CPU in a reasonable amount of time you're not really using very complex models. Mac is good for many things and straight up horrible at mostly everything else, like not even a compromise horrible, just can't do it horrible. My mbp does not and will not probably ever have the firmware to use it's GPU for modeling."
50,jpmq9nr,14iid79,1,You can set your device programmatically and even ask chatgpt to do that for u. But yeah I mean if u are serious about ML u get a remote Linux workstation
51,jq3qlp9,14iid79,1,"Oh, how this subreddit has fallen."
52,ka6lbf7,180lo6d,2,Is there a chance to check if any of the have a high percentage of resemblance to a training data point?
53,ka7qtbp,180lo6d,2,Questions. Is there a likelihood that the generated face can look like a real face? I imagine there is no tech to filter for that? Is that ever discussed in this dev space?
54,ka6nmaf,180lo6d,1,"This is just a little project I made. It's nothing super fancy but I had the idea to make it after I discovered ([https://thispersondoesnotexist.com](https://thispersondoesnotexist.com), which was not working at the time). It was made using StyleGAN3 and StyleGAN2.

Let me know if you like it: [https://thisfaceisfake.com/](https://thisfaceisfake.com/)"
55,fgn2vu5,ezg8sr,0,Thx
56,mibxsr0,1jdko1z,45,Obvious ad is obvious
57,miceepk,1jdko1z,37,Garbage ads for some random services with nothing to do with deep learning. Still 50 upvotes for some reason. Hello bots
58,micypbn,1jdko1z,16,r/LinkedInLunatics
59,midlaum,1jdko1z,8,Why here tho? We‚Äôre not a marketing subreddit
60,migh4ke,1jdko1z,3,Those pictures have very obvious processing artifacts.
61,mijhuf5,1jdko1z,3,Wrong sub
62,mikh1oc,1jdko1z,-1,I will use all these icons. Thanks.
63,mi7z674,1jd6c0v,9,"Why is this particular animation useful?  The CNN layers in particular introduce 2d filters that ""slide"" around over the input tensor data and consume information about low level shapes that are then passed to deeper layers. Something like  input > Eye, beak, toe -> body, feather, head -> (dense layer) bird -> (dense layer) -> Bluejay.

A more helpful visualization (which I'm 99% sure is already out there somewhere) is visualizing what core shapes are being discovered by those CNNs as they pass information. It would be like rotating the above visual horizontally by 90 degrees and spending time showing how the various CNN filters pluck out details, then shift the z-axis deeper into the net as it moves to the next layer."
64,miael05,1jd6c0v,1,the hell is that shit
65,liuakpe,1evuwr6,27,"""A plan is not a plan that doesn‚Äôt work 100% of the time, it is just an idea.""

lol

Not that you have any idea what you are trying to talk about."
66,lixvm6q,1evuwr6,6,">>> How come LLM responds in constant time even for polynomial or exponential problems?

1) They do not inference in constant time. It's \~ O(n\^3) by output tokens count. 

2) Moreover in order to generate next token LLM should process all previous tokens, and repeat this procedure for every next token.

3) Average human can remember \~7 things at once. Working memory of LLM is huge, not just huge, but very very ... very huge. They can solve many problems at once, which would take an ordinary person a lot of time to just build logical chains.

  
So, they can reason at some level, not huge, but sufficient enough to solve some problems and replace humans in some areas."
67,lj1ktuy,1evuwr6,2,"I love these ‚ÄúLLM gotcha‚Äù posts, yes LLMs fail at things, we haven‚Äôt achieved AGI, but we also have benchmarks that tell us exactly how good or bad they are at things and they are quite capable.

It‚Äôs not just memorization, they must be creating a world model to fit the information in the weights. This is quite similar to how humans learn"
68,liufupb,1evuwr6,6,"Yh the ‚ÄúIntelligence‚Äù thing is complete bulls***. That‚Äôs why I roll my eyes when I hear yet another idiot talk about AGI. LLMs are just search engines with an added ETL pipeline at the end, nothing more as of now."
69,liza1so,1evuwr6,2,"They have solved 0 novel problems, that would mean they are not really smart but highly efficient and accurate copy cats. More like a search engine that can mimic logic"
70,lj7az7f,1evuwr6,1,It‚Äôs very possible to achieve planing like behavior with a good enough heuristic. Deepmind recently showed that a sufficiently trained network could play grandmaster level chess without search. For a lot of situations an approximate plan might be good enough.
71,liuyrxz,1evuwr6,-1,only probability it knows
72,livm0qu,1evuwr6,0,‚ÄúLearning‚Äù
73,fvyksh9,hfkljz,8,But can it import numpy?
74,fvyagbq,hfkljz,8,My $200 1060 still going strong
75,fvy4y5t,hfkljz,27,So?
76,fvzcjsb,hfkljz,3,Can it run Doom?
77,fvyqq7r,hfkljz,2,[deleted]
78,fvy58sd,hfkljz,1,"I hope you make the best out of it.Conversational AI hmm. Are you a PyTorch person or TF?

We could maybe collaborate on some projects. I might not have access to such a beast but I do have access to a server with a couple of GPUs"
79,fvzlfpl,hfkljz,1,ok
80,m9fvvr6,1ib7g75,23,">how is that possible unless they are trained on same data or the weights are same, does anyone think same

They likely used ChatGPT's answers for finetuning/aligning.

They call it ""Reinforcement Learning from AI Feedback"", but I'm not aware of any published details about **what** AI DeepSeek used for that.

Seems natural to use OpenAI's models for that. If not exclusively, then at least as part of the ensemble."
81,m9hwy8v,1ib7g75,3,Isn't there a lot of crossover between the corpora used for training... if the algorithms are all similar too then you get similar outputs?
82,m9i2npb,1ib7g75,1,Yes
83,m9jwy9i,1ib7g75,1,Thank you China
84,l6blhj6,1d430jf,9,"only if you guarantee that I own the model after training, sick of getting scammed by you shitty researchers on a ""mission""¬†"
85,l6oeiom,1d430jf,2,[deleted]
86,l6chauy,1d430jf,2,Would I be able to get a citation on any publications you produce or direct payment? A charitable tax break at least?
87,l6ci7ld,1d430jf,1,Use amazon mechanical turk. No one's gonna do grunt work for you for free.
88,mqmt5b0,1keioz4,3,If something is too good to be true it probably is a scam.  I am very leery of this one.
89,mqqkl2g,1keioz4,3,Lol what kinda stupid dum dums fall for this crap?
90,map8rhq,1ifd5v5,2,"8xL40 has about 400GB of VRAM, I guess you could train 1-B model on context of up to 50k tokens.

Maybe a bit bigger model, but I dont think you will get away anything bigger than 2-4B parameters. Which is a model that barely talks, usually just reproduces language, but does not encode knowledge."
91,mauupnb,1ifd5v5,1,I would like to participate on this project!
92,mb79167,1ifd5v5,1,Interested
93,maf4skd,1ifd5v5,1,Interested‚úãÔ∏è
94,maf6qvf,1ifd5v5,1,"Hi,I want to be part of it, because I want to learn if it's possible can I join.

Thanks üôè"
95,mag7zuk,1ifd5v5,0,Let‚Äôs gooo! üöÄ
96,maf7r1t,1ifd5v5,0,"Join our community 
https://goto.now/EYQ1B"
97,mal2foy,1ifd5v5,0,"Word of advice. Given the market is saturated with general purpose (or coding) LLMs that are attempting to do well on as many industry benchmarks as possible‚Ä¶ I would tune this one for a niche. So it stands out on a particular topic space with narrower utility than the stuff we see advertised every day.

For example, base train on English texts, but fine tune for therapy / psych (if such a data set exists).  Ignore the industry benchmarks and have it evaluated by real people using likert scale questions on how helpful they feel it was. Collaborate with academia and have it a/b tested against chat based therapy sessions with a real person. Then capture the logs of it all and use it to fine tune it more (anonymized of course).

This is just one idea, but it could literally be anything.

Another random idea is ‚Äúfinancial adviser‚Äù agent."
98,mag1exy,1ifd5v5,-1,Hey!! I want to be a part of this!
99,mag2l23,1ifd5v5,-1,Interested
100,mahyz22,1ifd5v5,-1,I want to join in too
101,mai2uiv,1ifd5v5,-1,Interested
102,mal9ocu,1ifd5v5,-1,"Hi, can I join this wonderful AI effort? Thanks!"
103,kc7jyfi,18c0kcm,51,"Your dataset is large enough that you have to pay attention to the efficiency of your code. If you have an inefficient algorithm, no language matters. You need to analyze your code and see where the bottle neck is and optimize it. Your problem doesn‚Äôt appear to be the language based on the description of your question"
104,kc7rq77,18c0kcm,18,"For loops in pandas and iterrows are painfully slow. Without seeing your code it's a bit difficult to give you more concrete advice but 4 things that can help: 

- vectorize your for loops if possible
- use pandarallel.parallel_apply for multiprocessing 
- change iterrows for itertuples using only the columns you need 
- consider using Polars Dataframes instead of Pandas. Their performance and memory usage is much much better."
105,kc7mgmg,18c0kcm,6,"Have you profiled your code? You can see which parts of the code take the most time and optimise those parts. You don't need to use the whole data set, just cut out a reasonable-sized chunk."
106,kc8vba0,18c0kcm,5,"Pandas is built on numpy, and numpy is written in C, extremely efficient. You are just doing for loops which you should never do for this large scale of data."
107,kc7kqdw,18c0kcm,7,"If you need some Operations on it use cudf.
If you need to query, learn SQL or use parquet instead of pandas dataframes.
Also using a jupyter Notebook instead of a proper py script is slower."
108,kc7szv0,18c0kcm,3,You can use sql for processing data. Sql queries can also be executed within notebooks. Sql is optimised for large dataset.
109,kc8n5hh,18c0kcm,3,"As the comments point out, people too often go ""Python is so inefficient and slow. Help me find a different language"". When in reality they should instead fix their code. Using `iterrows`, as you mentioned in a comment, in itself is a red flag 99 out of 100 times when it comes down to performance.

Probably best for you to post your code on Stack Overflow or Code Review for efficiency suggestions."
110,kc7sxly,18c0kcm,4,"Julia is great https://github.com/JuliaData or https://github.com/sl-solution/DLMReader.jl might be a good startingpoint

CSV can be pretty fast if you call it correctly with multiple threads and the right args for the columns and delimiters and stuff.

Then you get a dataframe object which is also quite efficient to work with."
111,kc80ni9,18c0kcm,2,You can check Polars as python library or switch to Spark framework for more parallelism
112,kc8f4du,18c0kcm,2,"Hi, I switched to working using Pyspark for large datasets. It's very easy to pick up as well."
113,kc8jnwo,18c0kcm,2,Just don't use the for loop.  Your dataset is actually very small.  Use .apply (pandas dataframe) or .map if you use custom function to process data.  Regular expression is very slow.
114,kc8gh8t,18c0kcm,1,"Use Nvidia libraries like cudf, cupy"
115,kc9k74y,18c0kcm,1,"Profile the code and use something like snakeviz to understand the bottleneck. For most data analysis task, changing the language doesn't being much benefit since the core is likely implemented C or C++."
116,kc9kfjm,18c0kcm,1,"I'm curious - I don't see 7.3 million rows and 7 columns to be particularly large.  What is the type of data?  what machine specs are you running?  What version of python?  version of pandas?  have you considered Polars?  Python is 'pretty good' at everything...  I say that because it's probably good enough that a language change may not solve your problem.

&#x200B;

before doing anything, use a more current of python, upgrade pandas or switch to Polars."
117,kcayz3g,18c0kcm,1,"Use torch, you can run instantaneously on GPU."
118,kcbz2gj,18c0kcm,1,show us the code
119,kcgl9au,18c0kcm,1,"If I could see it or a facsimile of it I could help. Otherwise, taichi, numpy hit, etc it goes all the way down like you're basically writing C. 
I am suspect that is the issue here. Vectorize."
120,kch6gk0,18c0kcm,1,Use polars instead of pandas
121,kckstjw,18c0kcm,1,https://github.com/huggingface/candle
122,kcmfsoj,18c0kcm,1,Use Polars. It's written in Rust but has a Python frontend. Should be much faster. If you're using pandas.apply() or something that's notoriously slow.
123,kcn6koj,18c0kcm,1,"You can use python with pyspark, great for big-data and machine learning etc. you will need to locally install spark or use a containerized version of with with Docker"
124,jila0vu,131naf8,1,http://ainsider.crd.co/
125,ji1h35i,131naf8,1,I am looking forward to the GPT extension where 200 years of actual cookbooks have been digitized and modeled.
126,ji2wl2e,131naf8,1,Yeah i admit i'm using chatgpt for this
127,epz42az,bwmbsy,13,"""it is true that our model *could* be overfitting to this data set and *might* not generalize to new data very well. That being said, I‚Äôve got a feeling these agents are learning quite a bit more than simple curve fitting, and as a result, will be able to profit in live trading situations.""

Great. All this science to end up trusting what ? a feeling ?

Although you spend some time on the stationarity problem (which is good), a big generalization problem could be that you do not mention any cross-validation, and this is a real challenge using time series.. and RL..."
128,epz8ed6,bwmbsy,2,[deleted]
129,epznlh0,bwmbsy,1,"You need to test the model on new data, without that this could be (and probably is) just one big overfit. If you have data from 2015 to mid 2019, split it from 2015 to 2018/ mid 2018 for training, mi d 2018-2019 for hyperparametrs optimization and then test it on 2019 to mid 2019 data. If you preform well on last segment congratulations you will be rich, if not you are just of the many that failed."
130,jbs4glr,11oct9z,6,wrong sub
131,frcj0a4,gny0d6,11,This guy is the one who copied my video and uploaded to his channel. I am sure this video is also of any other not his own content ! üòî When i commented in section he removed my video. Please support and report him.
132,frdc8o3,gny0d6,8,I doubt any AI/ML was used to implement this...
133,l2xvhqb,1cm17nb,17,"because, we humans do NOT have consistent colour patterns in real life-

we paint some buildings in brown, red, orange, green or whatever hue we like

  
we paint doors in hundreds of shades of brown, black , yellow or whatever we feel like

It is just confusing for computers to understand that fact humans are so inconsistnt and want them to guess accuratly each time what it should be

how often would a human be able to guess the real colour from a black and white image? Why do we expect computers to do it very well? 

Compters can do it better than most humans, just not as well as out high expetactions of decoding 3 bits of info (colour) when we share 1 bit of info (black and white)"
134,l2y3v6k,1cm17nb,6,"The rest 20% of the work is the hardest. If you want consistent colors your model needs to remember everything that was colored before. 
If you want historical accuracy it needs to know history, understand what's happening on this image, when it happened, where it happened, the season and a lot of other details."
135,l2z6n3z,1cm17nb,2,IA?
136,l32jkri,1cm17nb,0,"Or maybe there's no enough investment for this. 
It doesn't interest media or billionaires to invest in"
137,mnq4tyj,1k1jlq1,6,"I had a good experience with them, they helped me with editing my research paper)"
138,mnn15fr,1k1jlq1,2,Yeah they scammed me and ‚Äî never replied when ‚Äî I tried to contact them ‚Äî Stay away from them!
139,mph3so9,1k1jlq1,1,"I actually used LeoEssays not too long ago when I was panicking over a last-minute research paper (classic me thinking I could manifest the deadline away)üòÖ Honestly, I was impressed ‚Äî their writing service was super easy to navigate, and they really paid attention to the details I included in the instructions.

When I needed someone to write my essay, I wanted it done fast and properly, and LeoEssays delivered on both fronts. The paper came back well-organized, no plagiarism drama, and even the citations were spot-on (which is where a lot of other services sometimes flop). Pricing was fair too ‚Äî not ""sell-a-kidney"" expensive like some places I've seen.

I'd definitely use them again if my procrastination skills kick back into high gearüòÑ Hope this helps, and good luck with your paper!"
140,mq0kqwt,1k1jlq1,1,"I tried Leoessays recently, and everything went great! I ordered a paper with a tight deadline, and it was delivered on time. The quality was top-notch too ‚Äî no issues with plagiarism or formatting. If you're looking for the best assignment writing service, I‚Äôd definitely recommend them!"
141,mq0lfwb,1k1jlq1,1,"I recently tried Leoessays for an upcoming paper, and honestly, everything went great! I was a little skeptical at first (I mean, it‚Äôs always hit or miss when trying a new paper writing service online), but I was pleasantly surprised. The turnaround time was quick, and I was able to get my paper on time without any issues."
142,mr8it1y,1k1jlq1,1,"I haven‚Äôt tried Leoessays myself, but I totally get wanting to do some research before trusting a writing essay service. I‚Äôve had some mixed experiences in the past, so I usually check reviews and get a feel for the site first."
143,mr8j268,1k1jlq1,1,"Also, pro tip: If you‚Äôre in a pinch and procrastination is real, using a solid college essay writing service is a total lifesaver. No shame in getting that help when the stress is real."
144,lznpilh,1h323w4,27,"Models are not learning on ‚Äúnever-ending‚Äù data. Training still happens offline on specific datasets, with maybe slow and well-controlled iterative updates¬†"
145,lzs34ba,1h323w4,3,It's called steps. It refers to the number of gradient updates and comes alongside the effective batch size.
146,lznnd3y,1h323w4,4,"To be honest, epochs were always useless. I don't know why libraries were built around epochs. 

The problem is that the number of iterations (back propagations) in an epoch changes depending on dataset size and batch size. 

For example, if you train a model with batch size 100, and the dataset is 100 samples, then 10 epochs is only 10 iterations. If you train ImageNet with 1.3 million samples, 10 epochs is 130k iterations. In the first case, basically nothing will be learned because it hasn't had time to.

The alternative is just use iterations (which I would argue is more fair and makes more sense anyway). Back in the day, before keras and pytorch, we used iterations. Even to this day, I still use iterations (I calculate the number of epochs to train based on epoch=iteration*batch/dataset)."
147,lzocceu,1h323w4,1,"I'm with you. I am working on a hobby ML framework that operates on streaming iterations instead of epochs. I still have epochs in the calling code above, but it's more of a high level eval decision for a specific dataset than a built in requirement. Ultimately I plan on trying to wire my network to streaming audio/video sources."
148,l96lf8h,1diu4gh,14,Well I think even writing this out you can see that you probably want to start coding by yourself immediately if you want to have any engineering role.
149,l9689r7,1diu4gh,26,"I think it is fairly simple, just stop the subscription to chatgpt or gemini, and start writting for yourself.

I think I also had something similar, I prompted for easy stuff and got lazy.."
150,l96cwnb,1diu4gh,7,You only learn by practice.
151,l97o18x,1diu4gh,9,"Get these LLMs to generate 10-15min code exercises for you aimed at improving your weaknesses (not just reinforcing your strengths), with increasing difficulty as you progress"
152,l97vcau,1diu4gh,7,Stop using gpt and start practicing
153,l99p0l6,1diu4gh,2,"During undergrad, I was trained to not even use an IDE. Just a plain text editor without any of the autocomplete plugins (I used to use Sublime). Yes, you‚Äôll end up consuming more time when you do things manually this way but you‚Äôll learn a lot compared to speeding up everything. The search for an answer in Stack Overflow or some other forums would point you to different answers but each could provide you with something different, something you might need in your other problems someday. Something you can‚Äôt get from LLM chats. 

To note, my undergrad years were 2014-2018 then I proceeded with my Master‚Äôs on 2018-2021 while working a full-time job. 

There‚Äôs a paper a few months back saying that dependence on LLM dulls someone. It‚Äôs published in Nature (or sub journal of it) but it‚Äôs behind a paywall."
154,l97d1qz,1diu4gh,2,also grammar.
155,l9aw0pp,1diu4gh,1,How do you find an intern if you can‚Äôt write code?
156,l9s6az6,1diu4gh,1,"1. No one cares how you wrote the code, as long as it works. Take it slow, practice, read.

2. If you sistematically can‚Äôt remember syntax despite practice, you may benefit from a clinical neuropsychogist visit. This could be dyslexia, for example.

You can do this!"
157,l98ff5r,1diu4gh,1,"No harm in learning to do it yourself. But using LLMs to fast track this stuff is the future, so you‚Äôre not really in the wrong"
158,lgsa475,1elkjfa,33,"I wasted 6 years on TensorFlow, don't make my mistake, start with PyTorch, you'll save a lot of time and effort"
159,lgsd2sz,1elkjfa,11,Pytorch. Tensorflow has more readable code but Pytorch is better.
160,lgsdufv,1elkjfa,9,even google has switched to JAX from TF
161,lgsqc6i,1elkjfa,8,"I used both as a TA and current phd student and pytorch is better, fight me!"
162,lgs9kk7,1elkjfa,6,[deleted]
163,lgsz96j,1elkjfa,4,"Have you tried searching reddit? This question has been asked a gazillion times already on multiple subs (datascience, machinelearning, deeplearning). Why wait for folks to answer when all the possible answers and more have already been posted."
164,lgucj0q,1elkjfa,2,PyTorch any time
165,lgwbg2k,1elkjfa,2,I think what you should have asked is Jax vs PyTorch.
166,lgwrqzb,1elkjfa,2,"Sorry, who‚Äôs asking this question in 2024?"
167,lguu7oo,1elkjfa,1,"Also if you ever want to work with huggingface models, it‚Äôs PyTorch all day long. 

I don‚Äôt particularly like the syntax of it, but it‚Äôs decent enough"
168,jocmkeo,14ars0t,4,Can you stop spamming the sub with this crap? One time was enough.
169,joc1tuj,14ars0t,0,"Ok, but where's the original video?"
170,jobvmmz,14ars0t,-5,Tutorial Link: [https://www.youtube.com/watch?v=GPop6IFaLQE&t=6s](https://www.youtube.com/watch?v=GPop6IFaLQE&t=6s)
171,joe90t2,14ars0t,1,Let me guess.. the fake one is both!
172,j5loz4p,10jixci,3,This honestly just looks like really shitty photoshop
173,j5lbbfx,10jixci,4,So he also sweat through all of them? Including the leather jacket? Thats some impressive sweating.
174,j5l15jd,10jixci,0,By using what?
175,j5lpnc5,10jixci,1,oh my goodness the ears
176,j5mlxvb,10jixci,1,Kinda lame content for this sub
177,iy7kgtw,z7nt9o,31,"Researchers know that, but it does not help in any way to better understand DNN. A bunch of DT is not more explainable than a DNN"
178,iy7ncq9,z7nt9o,5,"Maybe, a decision tree is an example of a NN? I mean that NN is more generic structure because it may include an arbitrary Neuron design and custom layers design?"
179,iy7ielr,z7nt9o,10,"Thanks for pointing out the article, it‚Äôs going to be useful for a lot of people.

Anyway, when we refer to the ‚Äúblack box‚Äù nature of DNNs we don‚Äôt mean ‚Äúwe don‚Äôt know what‚Äôs going on‚Äù, but rather ‚Äúwe know exactly what‚Äôs going on in theory, but there are so many simple calculations that it‚Äôs impossible for a human being to keep track of them‚Äù. Just think of a simple ConvNet for MNIST classification like AlexNet: it has ~62M parameters, meaning that all the simple calculations (gradients update and whatnot) are performed A LOT of times in a single backward pass.

Also, DNNs often work with a latent representation, which adds another layer of abstraction for the user: the ‚Äúreasoning‚Äù part happens in a latent space that we don‚Äôt know anything about, except some of its properties (and again, if we make the calculations we actually **do know** exactly what it is, it‚Äôs just unfeasible to do them).

To address these points, several research projects have focused on network interpretability, that is, finding ways of making sense of NNs‚Äô reasoning process. [Here‚Äôs a review](https://arxiv.org/pdf/2012.14261.pdf) written in 2021 regarding this."
180,iy7fk3v,z7nt9o,5,"[https://www.youtube.com/watch?v=\_okxGdHM5b8](https://www.youtube.com/watch?v=_okxGdHM5b8)

&#x200B;

Discussion of the orignal paper."
181,iy7xvbr,z7nt9o,3,"Was interested when I first head about this concept.  People seemed to respond with either thinking it was ground shaking, ‚Ä¶..or alternatively that it stood to reason that given *enough* splits it would be the case!  Do you think though,  that from a practical usage perspective this doesn‚Äôt help much because there are so many decisions‚Ä¶. Article has a lot more than just that though and a nice provocative title."
182,iy8lyyf,z7nt9o,2,"Any ML classifier or regressor is basically a function approximator.

The function space isn't continuous but rather discrete, discretized by the dataset points. Hence, increasing the size of dataset can help in increasing overall accuracy. This is relatable with Nyquist criterion. Less data and its more likely our approximation is wrong. Given the dimensions of input space and range of each input variable, the dataset size is nothing. E.g. for 224x224 rgb input image, input space has total pow(256, 224x224x3) possible input values, which is unimaginably large number, mapping each to a correct class label (total 1000 classes) is very difficult for any approximator. Hence, one can never get 100% accuracy."
183,iy8x636,z7nt9o,2,Wonder why if so NNs are so much better on unstructured data while being trickier and in general more useless on structured data compared to tree based classifiers and boosted classifiers.
184,iy9lq76,z7nt9o,2,"I think that NN is general structure algorithm can learn anything from data depends on problem and it can approximate between data distributions , such automa NN is good example"
185,iya0x5i,z7nt9o,2,You can just cite the original paper
186,iya34at,z7nt9o,2,Correlation is not causation.
187,iyaux7r,z7nt9o,2,"A deep neural network can approximate and function.

A deep recurrent neural network can approximate any algorithm.

The are mathematically proven facts.  Can the same be said about ‚Äúa bunch of decision trees in hyperspace‚Äù?  If so, then I would say ‚Äúa bunch of decision trees in hyperspace‚Äù are pretty darn powerful, as are deep neural networks.  If not, then I would say the author has made a logical error somewhere along the way in his very qualitative reasoning.  Plenty of thought experiments in language with ‚Äúbulletproof‚Äù arguments have led to ‚Äúcontradictions‚Äù in the past, only for a subtle logical error to be unveiled when we stop using language and start using mathematics."
188,l06ea24,1c770nz,14,"Echoing everybody else's advice, take a break. However, I want to give a slightly different reason: You will always feel like you are a little bit behind, and that is okay. In fact, the sooner you teach yourself that feeling a little bit behind all the new stuff coming out is acceptable, and that taking breaks is not only acceptable but also normal and good for you, the more successful you will be at keeping up. Let me explain.

I have been in a PhD focused on AI for 8 years. I am about to finish that PhD. Before that I worked in robotics for 3 years. Before that I did a masters program in robotics. I can tell you that at no point in my journey have I ever felt ""caught up."" For many years this was alarming and constantly unsettling. Like you have described, I felt a pressure that if I wasn't constantly keeping up with new techniques, models, and achievements I would fall so far behind I would never catch up again. What I've learned in the last few years is that you never get fully caught up, but that's okay because no one else does either. Having conversations at conferences like NeurIPS, RSS, ICRA, AAMAS etc, You start to realize that even people responsible for the most impressive achievements in one area have no idea what's going on in other areas. I remember in one conversation at one of these conferences last year, I got to explain to a multi agent learning best paper first author all that was going on with NeRFs, diffusion, Gaussian splatting, and its implications for robot mapping and coordination. It was a illustrative experience of how someone awarded for moving deep learning forward was still ""behind"" on what is changing in deep learning.

Even the best of us don't know everything. The most valuable thing you can do is be proud of what you know at this moment, assure yourself that as long as you keep trying you will keep learning and improving in the future, and that taking breaks for mental health is a critical step in constant learning and improvement. ![gif](emote|free_emotes_pack|grin)"
189,l05w4j5,1c770nz,10,"Take the break, come back refreshed"
190,l064881,1c770nz,3,"The important ideas of deep learning aren‚Äôt changing every week or two. As long as you persist in the fundamentals (which is what I think gives the competitive advantage in any field), a break will help you to think better and learn more efficiently."
191,l066vve,1c770nz,2,"You need to take breaks to avoid burnout. Your knowledge won't become obsolete. And after a break you should have more energy, thus you'll be more effective. Remember that learning has to be enjoyable - you can't feel joy when you're tired"
192,l08pghm,1c770nz,2,"Do something completely different that you've never done before. Something that sounds fun. 

I like hybrid models so that's what I do when I get bored. Yours could be completely random. It should be a little below your skill level if you still want to be a little challenged with some parts but have it be easy+ level, or a little past your capabilities if you learn better that way and have the emotional energy. That way you not only are learning but it's fun and you can find clever ways to use what you know on stuff you already know. 

Also, dumb ways to use the knowledge work almost as well as the clever ways. You can save a little mental energy by not requiring cleverness, when cleverness just won't do.  

Just require yourself to at least vaguely think about  how you can use the new knowledge of programming, theory, or task-adjacent knowledge to be useful by solving any problems you have or might have later.

2000 lines later I can go back to the other stuff and muscle through the boring parts of learning necessary but not cool stuff. Might even use something I learned to make the boring thing easier, better, and occasionally to make the boring thing wicked cool."
193,l09gs5f,1c770nz,2,"I think some breaking points might happen in at least 6 months or more. The rest is deviation, new applying and stuffs like that. Mamba does not release in 1-2 years, but strong continuously developing in 3 years, the rest will be same as well. Take your break."
194,l08r2r8,1c770nz,1,‰∫ãÂÆû‰∏ä‰Ω†ÂÆåÂÖ®Ê≤°ÂøÖË¶Å‰∏∫Áü≠ÊöÇÁöÑ‰ºëÊÅØÊÑüÂà∞ÁÑ¶ËôëÔºåÂú®‰∏≠ÂõΩÊúâ‰∏ÄÂè•‰øóËØ≠ÔºöÁ£®ÂàÄ‰∏çËØØÁ†çÊü¥Â∑•„ÄÇÁü≠ÊöÇÁöÑ‰ºëÊÅØÊòØÂøÖË¶ÅÁöÑÔºå‰πüÊòØ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ÂºÄÂßãË°åÂä®„ÄÇ‰∏äÂ∏ù‰πüÊó†Ê≥ïÈòªÊ≠¢Êñ∞‰∫ãÁâ©ÈöèÊó∂ÂèëÁîüÔºåÂõ†‰∏∫ËøôÊâçÊòØÁîüÊ¥ªÁöÑÊÑè‰πâ„ÄÇ
195,kozpm2t,1aj8fqg,13,"What it means is, feature engineering is more relevant in traditional ML while it isn‚Äôt in DL. This is because your NN is going to learn more complex features than anything you can hand craft. 

That said feature engineering is a broad term and a lot of things used for preprocessing data in deep learning can be referred to as feature engineering."
196,kozpzur,1aj8fqg,2,"It's all covered here:

[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)

In some cases you need labels: that's called supervised learning. In others you don't. That's unsupervised learning."
197,kozk0r7,1aj8fqg,-5,Look into supervised and unsupervised learning
198,koznunj,1aj8fqg,0,"I think you need to first differentiate between the dataset that's used for training, and the inputs used to tweak the process during training--aka supervision. All AI training/deep learning/machine learning requires a large amount of data to train on. However, advanced AI training methods like deep learning allow for decreased inputs from human programmers during the training process--ie, the process goes from being supervised to ""unsupervised"". When you talk about no human input during deep learning that's probably what you meant."
199,kozikkf,1aj8fqg,1,"Indeed you need to train a model before you can use it, and you train it with tons of data. Oncr you have a model you can use it to solve simmilar tasks as the ones you used to train it, so there is no midel without data in deep learning. Also you need to create the model in the way you hope it works better"
200,hi5g3g0,qg5gl8,1,"Not gonna read this, but top notch title right there. Kudos."
201,ftr0v5m,h7832v,1,I am gonna steal your setup though
202,ftr9v1x,h7832v,1,Usually I like to fill my models with hot air
203,f0aqdk7,d4f51v,-2,[deleted]
204,ky504c1,1bwbsgk,38,"I used to use Matlab during uni days but not anymore. The future will not need an overpriced software when better free alternative exists. Learn to setup your environment, it'll take you as long as it takes you but you'll be rewarded afterwards."
205,ky55k1m,1bwbsgk,15,We don't hate you. But your language hates you. And you keep going back for more abuse..
206,ky52lci,1bwbsgk,23,Cringe
207,ky50f6m,1bwbsgk,6,Let it go man. Join the light side before it is too late
208,ky5utf2,1bwbsgk,5,I was forced to use matlab for years and it made me dumber
209,ky61ayw,1bwbsgk,6,Yes
210,ky68r54,1bwbsgk,5,yes
211,ky6ewtc,1bwbsgk,4,"You resist using the most popular, well supported tool because you dislike the interface.

Man-baby seems apt."
212,ky53x95,1bwbsgk,4,Just go immediately use pytorch
213,ky59vkk,1bwbsgk,5,"oooof. yea, I‚Äôd have a hard time working with you."
214,ky73tz5,1bwbsgk,3,"I was forced to use matlab in one of my grad school courses. I loathed every picosecond of it. Weirdly enough, I‚Äôd rather use octave against matlab, which is kinda like or basically just the open source version of matlab. lol

Might also be because our entire class hated our professor in that subject."
215,ky7pv64,1bwbsgk,2,"Try Visual Studio Code and its python plugin.  Or use pycharm.  I also don't like that it doesn't let you easily explore and manipulate workspace variables, but I recently made the jump after 25 years of Matlab use in academic and professional robotics work.  

AI has helped with the transition too.  I can easily just ask claude to tell me about how I can achieve the same thing from some matlab code in python.  

I used to always just code high performance stuff in C++ and then do high level testing of algorithms in Matlab.  I still do a bunch of that, but I needed to learn Python to teach it as a class to students because I didn't want to have to purchase a site license for Matlab any more when I found myself only using base matlab and no real toolboxes.

As a teacher, I have enjoyed seeing Github Copilot's predictive code do a really neat job at auto-completing whole functions in python.  As I understand it, Matlab is sliding further down the IEEE list (though it is still around the #20 spot).  Probably getting to be time to move on."
216,ky8bkzy,1bwbsgk,2,Yes
217,ky8mu12,1bwbsgk,2,"Matlab had a big head start in the early 2010s with deep learning. Alexnet itself was created in matlab.

Mathworks was yawning rather than developing their libraries over the last decade or so."
218,ky8slb8,1bwbsgk,3,"Python coders don't even know you exist man. Just take it easy learning the tools and syntax, and you'll be fine."
219,kya190n,1bwbsgk,2,"Yes, and fuck em'. We niche society of reddit are freaks. At the end of the day, we're given a job, and it's solved via programming. Noone outside reddit cares how it's programmed."
220,ky5ov36,1bwbsgk,2,Edgy
221,ky5ycbe,1bwbsgk,0,"You are not alone that Python is not necessarily the optimal. Andrew Ng posted this [https://www.deeplearning.ai/the-batch/issue-238/](https://www.deeplearning.ai/the-batch/issue-238/)

""I think the complexity of Python package management holds down AI application development more than is widely appreciated. AI faces multiple bottlenecks ‚Äî we need more GPUs, better algorithms, cleaner data in large quantities. But when I look at the day-to-day work of application builders, there‚Äôs one additional bottleneck that I think is underappreciated: The time spent wrestling with version management is an inefficiency I hope we can reduce. ""

Another bottleneck is hardware integration, because the code must be compact and efficient to reduce energy consumption yet must operate in real time, and requires extensive testing before real-world deployment. MATLAB have solid track record in the industry for that. 

Rather than MATLAB vs. Python, the better question is how we can use them together. MathWorks is building the bridge between the two worlds. 

https://www.mathworks.com/help/deeplearning/networks-from-external-platforms.html"
222,fzrw77i,i0m1e6,2,">\- In its current state, DL lacks causality, commonsense, intuitive physics, goal/intent understanding

What makes you think that's necessary?

>\- Roads and infrastructure need to undergo changes before SDCs can be deployed with reliability

Why? Humans can do it without any changes, so we can say for sure the task is solvable without any hardware exterior to the car.

>\- Legal hurdles need to be overcome

That's not a technical issue.

>\- Human drivers make mistakes too, but our mistakes are much more predictable than SDCs (e.g., human drivers don't drive into parked firetrucks and overturned cars)

Are they? The mistakes may be different, and of course as humans, we tend to forgive human mistakes easier. That doesn't mean AI mistakes will be more frequent or worse.

&#x200B;

>This suggests further training its deep learning algorithms with the data it is collecting from hundreds of thousands of cars will be enough to bridge the gap to L5 SDCs by the end of 2020.

Did he really say that? I don't think he claimed it's just more training. Nothing stops Tesla to deploy any software they want, nothing limits the hardware to deep learning only."
223,fzsbi9i,i0m1e6,2,"Common sense, intuitive physics, etc are all teachable imo"
224,fzr3jb1,i0m1e6,1,[removed]
225,fztlz4o,i0m1e6,1,‚ÄúThose who say it‚Äôs impossible should get out of the way of those who are doing it‚Äù
226,mjl24cj,1jj7tor,17,You can rent an A6000 for like $0.80 / hour on lambda labs.
227,mjleyu9,1jj7tor,4,"Around six months ago one of the macro guy (we are a HF) asked how to gauge the top is in for tech stocks on AI mania. One of our sell side guy nonchalantly said when you have folks advertising to get rid of their h100 inventory around one dollar an hour. 

We are getting there."
228,mjn0jvr,1jj7tor,1,Very expensive
229,g38qytb,iis7vp,2,">hardly distinguishable from the original sound

By a deaf person? 

Is this just an excuse to promote your low quality youtube channel?"
230,g38y7b2,iis7vp,1,Is this a video on how to run Python on colab? Probably should change the title.
231,mkn8f7n,1jnwjwf,17,"All of those are just tools, you learn them on the go when you need them. Doing ""a deep dive"" into pandas is a waste of time, partly because you'll forget all of it in no time, partly because tools are changing all the time. Familiarize yourself with what they can do on a high level and move on."
232,mknaj1w,1jnwjwf,5,"If you want to do a PhD you should be reading and implementing papers, not reading software documentation."
233,mknz35h,1jnwjwf,3,U don't need to know the libraries to do a phd. Just read papers
234,mkxqmzi,1jnwjwf,2,"Personally I think bechelor degree is enough. Getting a master degree or PHD is not necessary if you spend enough time doing research or just playing around with models and reading papers in ML area. 

But PHD with publications is a strong point for job hunting. Espeically if you want to do research for a living.  
  
Moreover, production experience matters. I would say try to get some intern opportunities in those big tech companies in ML field, you will learn and grow very fast. Once you done that, I bet you will have a clearer picture to whether going to grad school or not."
235,mknm9j0,1jnwjwf,1,"Never is enought, do what ever You want learn everything but hace one goal and that hola learn more than others"
236,mkq7in1,1jnwjwf,1,"Learn as you go. No human ever sat down to read the documentation(s) and learn stuff. That's madness. Learn what you need and keep moving. Tackle bigger and more ambitious projects than the last one, learn more advanced tools and implementations, and repeat. Read research papers as others in this thread have said, and keep moving."
237,lrlogik,1g201vh,3,Why is it here and not in a stats subreddit?
238,joa7zfy,14aevdi,5,"Check this out:

[https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)

[https://www.aime.info/blog/en/deep-learning-gpu-benchmarks-2021/](https://www.aime.info/blog/en/deep-learning-gpu-benchmarks-2021/)

google search ""GPU benchmark machine learning"" 

&#x200B;

They  benchmark various GPUs and their performance on machine learning models."
239,joabpkb,14aevdi,3,"The math says colab almost always just for doing small training. But the second you get into doing anything RL and go for weeks on end your better off with a desktop.

My 2 cents."
240,joakv9t,14aevdi,3,"Are you new to DL and deciding if you want to do it more seriously? If so I would 100% do colab. The GPUs it provides are pretty damn nice when you pay for pro, and it‚Äôs very reasonable for learning. Bias plug I wrote a [book](https://www.amazon.com/Inside-Deep-Learning-Algorithms-Models/dp/1617298638/ref=nodl_?dplnkId=80dd168e-005b-4d95-bfef-5b2d5be46ee9) teaching DL using only colab for compute, and tricks from fast.ai can extend the utility of just a colab session even farther. Once you feel like you really need to train longer, then it would make more sense to upgrade to a dedicated GPU that you own. It would also give you time to save up for one with more memory, as model size has been consistently increasing and make your life a lot easier."
241,jobmij8,14aevdi,2,"Though 3080 mobile GPU has 16gb of VRAM, desktop is more preferable if you need to train for longer period of time. You can also add additional GPU in future, which is not an option for laptop."
242,jobtpgn,14aevdi,2,"I‚Äôve worked in this exact situation for a while (laptop with 16GB 3080), in my view it‚Äôs:

- great for deep code, if you write libraries, have to debug,.. colab is just shite for this

- great for proof of concepts or small trainings (PEFL, fine tuning)

- a no go if you want to train at scale, but the same goes for colab, in that case you prep a job on your machine then send to the cloud

Colab would be best for a lot of experiments where you use existing code/checkpoints, you can‚Äôt beat the fire and forget aspect here.

I‚Äôve a ASUS Strix, works ok under Linux, no cooling issues whatsoever. If you go this route I would recommend a AMD CPU for cooling reasons, the GPU will produce enough heat by itself."
243,joi3u9t,14aevdi,2,Where can you get a 3080 16gb laptop for $600?
244,fruyd3i,gqsxzj,3,HTF does it have anything to do with deep learning?
245,frzthh3,gqsxzj,1,bruh this has nothing to do with deep learning. promote your shitty vids somewhere else
246,f612akv,dq4t9y,4,"> Here it prints a list of three values, R = 256, G=137 and B=125 are the intensity values.

Are you sure about that 256?"
247,elte09z,bh7ag2,1,Absolutely garbage article. Incredible amount of outright incorrect information.
248,e6we0i1,9k44n5,2,[deleted]
249,e6wa4jw,9k44n5,1,"Read this article for more details :-

[https://onclick360.com/unsupervised-machine-learning/](https://onclick360.com/unsupervised-machine-learning/)"
250,fc2otkn,efuzhp,44,"It might be just me, but that's exactly the irrational, scared muggle stuff I hope to not see in this subreddit"
251,fc3cgw5,efuzhp,10,What does this have to do with deep learning?
252,fc5s5v5,efuzhp,2,"One of my favorite sayings of late:

> I‚Äôm not concerned about what advanced AI will do to people.

> I‚Äôm concerned about what people who have access to advanced AI will do to people who don‚Äôt."
253,fc4qcr2,efuzhp,1,I played enough video games to see where it goes
254,m9mzkx7,1ic27bm,20,But twitterbros told me agi is here already
255,m9oqtaa,1ic27bm,3,Those easy tasks are for human üòÜ
256,m9re016,1ic27bm,2,Turn on the R1 thinking model
257,m9ol79d,1ic27bm,1,"AI, like all other technology is or will be used purely to feed the rich at the expense of the poor"
258,m9rewtu,1ic27bm,1,"Problems like this will go away once we ""solve"" tokenization"
259,m9t67vk,1ic27bm,1,"Honestly I have seen even worse from deep seek.
Wake me up when it can output content better than gemma2."
260,m9vo77a,1ic27bm,1,You have to click on the R1 button on the bottom!
261,m9nljze,1ic27bm,1,You can deal with such problem with regular expressions. The transformer architecture thought as a regular expression extractor is just not the right way to think of an LLM. It's an interesting problem though!
262,m9nan6e,1ic27bm,0,Why would it get it wrong and then get it right but in the wrong way
263,m9n5old,1ic27bm,-3,"I have a friend, 2x PhD +MD,  absolutely clueless about calculating percentages."
264,m9n1kc2,1ic27bm,-8,This is answered correctly now. All naysayers and dinosaurs who doubt AI capabilities will go extinct soon.
265,jbo1b4h,11nhy4m,2,How is this relevant?
